% Chapter title is in main.tex

The Riemann Hypothesis has inspired numerous attempts using classical methods from complex analysis and number theory. While these approaches have deepened our understanding of the zeta function and revealed fundamental obstacles to proving RH, none has yet succeeded. This chapter examines the major classical strategies, their key insights, and why they have not led to a proof.

\section{The Hadamard Product Approach}

The Hadamard product representation of the xi function provides one of the most direct paths to understanding the zeros of the zeta function.

\subsection{The Factorization}

Hadamard's theorem allows us to express the xi function in terms of its zeros. The complete zeta function $\xi(s) = \frac{1}{2}s(s-1)\pi^{-s/2}\Gamma(s/2)\zeta(s)$ has the factorization:

\begin{theorem}[Hadamard Product for Xi Function]
\begin{equation}
\xi(s) = e^{A+Bs} \prod_{\rho} \left(1 - \frac{s}{\rho}\right)e^{s/\rho}
\end{equation}
where $\rho$ runs over all non-trivial zeros of $\zeta(s)$, and $A$, $B$ are constants.
\end{theorem}

The convergence of this infinite product is ensured by the density estimate:
\begin{equation}
\sum_{\rho} \frac{1}{|\rho|^2} < \infty
\end{equation}

\subsection{Strategy and Key Observations}

The Hadamard approach attempts to leverage the product structure to constrain zero locations:

\begin{enumerate}
\item \textbf{Functional Equation Constraint}: The relation $\xi(s) = \xi(1-s)$ implies that zeros come in conjugate pairs $\rho, \overline{\rho}$ and symmetric pairs $\rho, 1-\rho$.

\item \textbf{Growth Control}: The order of $\xi(s)$ on vertical lines constrains the density and location of zeros. For $\sigma > 1$:
\begin{equation}
\log|\xi(\sigma + it)| \sim \frac{t}{2}\log\frac{t}{2\pi}
\end{equation}

\item \textbf{Real Part Constraints}: If RH fails, there would be zeros with $\Re(\rho) \neq 1/2$, affecting the growth rate asymmetrically.
\end{enumerate}

\subsection{Obstacles to This Approach}

Despite its appeal, the Hadamard product approach faces fundamental limitations:

\begin{remark}[Hadamard Limitations]
\begin{itemize}
\item The product representation alone does not obviously force zeros to lie on the critical line
\item Additional constraints beyond the functional equation are required
\item The transcendental nature of the relationship between the product and zero locations makes direct analysis difficult
\end{itemize}
\end{remark}

The product form reveals structure but does not provide sufficient leverage to determine zero locations precisely.

\section{Von Mangoldt's Explicit Formula}

In 1895, Hans von Mangoldt provided the first rigorous proof of Riemann's explicit formula, establishing a fundamental connection between the zeros of $\zeta(s)$ and the distribution of prime numbers. Edwards \cite{edwards1974} presents this as one of the cornerstones of analytic number theory.

\subsection{The Chebyshev Psi Function}

\begin{definition}[Von Mangoldt Function]
The von Mangoldt function is defined as:
\begin{equation}
\Lambda(n) = \begin{cases}
\log p & \text{if } n = p^k \text{ for some prime } p \text{ and integer } k \geq 1\\
0 & \text{otherwise}
\end{cases}
\end{equation}
\end{definition}

\begin{definition}[Chebyshev Psi Function]
\begin{equation}
\psi(x) = \sum_{n \leq x} \Lambda(n) = \sum_{p^k \leq x} \log p
\end{equation}
\end{definition}

\subsection{Von Mangoldt's Explicit Formula}

\begin{theorem}[Von Mangoldt 1895]
For $x > 1$ not equal to a prime power:
\begin{equation}
\psi(x) = x - \sum_{\rho} \frac{x^{\rho}}{\rho} - \log(2\pi) - \frac{1}{2}\log\left(1 - x^{-2}\right)
\end{equation}
where the sum is over all non-trivial zeros $\rho$ of $\zeta(s)$, taken in the order of increasing $|\Im(\rho)|$.
\end{theorem}

Edwards emphasizes that this formula makes explicit the connection between:
\begin{itemize}
\item The main term $x$ (expected number of prime powers)
\item The oscillating terms $x^{\rho}/\rho$ (deviations from expectation)
\item The zeros of $\zeta(s)$ (source of oscillations)
\end{itemize}

\subsection{Convergence and Interpretation}

The convergence of the series requires careful treatment:

\begin{remark}[Conditional Convergence]
The series $\sum_{\rho} x^{\rho}/\rho$ is conditionally convergent. Von Mangoldt showed that taking zeros in symmetric pairs $\rho, \overline{\rho}$ ensures convergence:
\begin{equation}
\sum_{\rho} \frac{x^{\rho}}{\rho} = \lim_{T \to \infty} \sum_{|\Im(\rho)| \leq T} \frac{x^{\rho}}{\rho}
\end{equation}
\end{remark}

\subsection{Connection to the Prime Number Theorem}

Von Mangoldt used this formula to provide a new proof of the Prime Number Theorem:

\begin{theorem}[Prime Number Theorem via Explicit Formula]
The fact that $\zeta(s)$ has no zeros with $\Re(s) = 1$ implies:
\begin{equation}
\psi(x) \sim x \quad \text{as } x \to \infty
\end{equation}
and hence:
\begin{equation}
\pi(x) \sim \frac{x}{\log x}
\end{equation}
\end{theorem}

\subsection{The Role of Zero-Free Regions}

Von Mangoldt's formula shows how zero-free regions directly impact prime distribution:

\begin{theorem}[Error Term Control]
If $\zeta(s)$ has no zeros in the region $\Re(s) > 1 - c/\log T$ for $|\Im(s)| \leq T$, then:
\begin{equation}
\psi(x) - x = O(x e^{-c'\sqrt{\log x}})
\end{equation}
for some constant $c' > 0$.
\end{theorem}

\subsection{Implications of the Riemann Hypothesis}

Under RH, von Mangoldt's formula simplifies dramatically:

\begin{theorem}[Explicit Formula Under RH]
If the Riemann Hypothesis is true, then for $x$ not a prime power:
\begin{equation}
\psi(x) = x - \sum_{\gamma} \frac{x^{1/2 + i\gamma}}{1/2 + i\gamma} + O(1)
\end{equation}
where $\gamma$ ranges over the imaginary parts of zeros on the critical line.
\end{theorem}

This gives the optimal error term:
\begin{equation}
\psi(x) - x = O(x^{1/2} \log^2 x)
\end{equation}

\subsection{Von Mangoldt's Contribution to RH Studies}

Edwards \cite{edwards1974} identifies several key contributions:

\begin{enumerate}
\item \textbf{Rigorous Foundation}: First complete proof of Riemann's heuristic formula
\item \textbf{Fourier Analysis}: Introduced Fourier inversion techniques to number theory
\item \textbf{Explicit Connection}: Made the zeros-primes relationship quantitatively precise
\item \textbf{Error Estimates}: Showed how zero locations control error terms
\end{enumerate}

\begin{remark}[Historical Note]
Von Mangoldt's 1895 paper was instrumental in establishing analytic number theory as a rigorous discipline. His techniques, particularly the use of Fourier analysis and contour integration, became standard tools in the field.
\end{remark}

\subsection{Modern Perspective}

The explicit formula remains central to RH research:

\begin{itemize}
\item \textbf{Weil's Positivity Criterion}: Reformulates RH using positivity of certain distributions derived from the explicit formula
\item \textbf{Li's Criterion}: Uses coefficients in the Taylor expansion related to the explicit formula
\item \textbf{Numerical Verification}: Modern computations verify the formula's predictions with extraordinary precision
\end{itemize}

The formula demonstrates that the Riemann Hypothesis is fundamentally about the optimal error term in the prime counting function—the zeros of $\zeta(s)$ control the fluctuations of primes around their expected density.

\section{Hardy's Theorem: Infinitely Many Zeros on the Critical Line}

In 1914, G.H. Hardy \cite{hardy1914} achieved a breakthrough by proving that infinitely many zeros of $\zeta(s)$ lie on the critical line. While this falls short of proving RH, it was the first result showing that the critical line contains a significant portion of zeros. Edwards \cite{edwards1974} provides an illuminating exposition of Hardy's original proof technique.

\subsection{Hardy's Original Method}

Hardy's proof employs a clever indirect argument using the properties of certain theta functions and their Fourier transforms.

\begin{definition}[Hardy's G Function]
Define the function:
\begin{equation}
G(x) = \sum_{n=1}^{\infty} e^{-\pi n^2 x^2}
\end{equation}
for $x > 0$.
\end{definition}

\begin{theorem}[Hardy 1914]
The Riemann zeta function has infinitely many zeros on the critical line $\Re(s) = 1/2$.
\end{theorem}

\begin{proof}[Proof (Following Edwards' Exposition)]
The proof proceeds by contradiction using the following key steps:

\textbf{Step 1: Connection to Xi Function}. Consider the function:
\begin{equation}
Z(t) = e^{i\vartheta(t)} \zeta\left(\frac{1}{2} + it\right)
\end{equation}
where $\vartheta(t) = \Im \log \Gamma\left(\frac{1 + 2it}{4}\right) - \frac{t}{2}\log \pi$.

This function is real-valued for real $t$, and $Z(t) = 0$ if and only if $\zeta(1/2 + it) = 0$.

\textbf{Step 2: The Contradiction Argument}. Assume, for contradiction, that $Z(t)$ has only finitely many zeros for $t > T$ for some large $T$. Then $Z(t)$ would have constant sign for all sufficiently large $t$.

\textbf{Step 3: Integral Transform}. Consider the integral:
\begin{equation}
I(a) = \int_T^{\infty} Z(t) t^{-3/4} e^{-at^2} dt
\end{equation}
If $Z(t)$ has constant sign for $t > T$, then $I(a)$ would be monotonic in $a$ for $a > 0$.

\textbf{Step 4: Taylor Series Analysis}. However, using the functional equation and properties of the xi function, one can show that:
\begin{equation}
I(a) = \sum_{k=0}^{\infty} c_k a^k
\end{equation}
where the coefficients $c_k$ change sign infinitely often.

\textbf{Step 5: The Contradiction}. The sign changes in the Taylor coefficients imply that $I(a)$ cannot be monotonic, contradicting the assumption that $Z(t)$ has constant sign for large $t$.

Therefore, $Z(t)$ must have infinitely many zeros, which means $\zeta(1/2 + it)$ has infinitely many zeros.
\end{proof}

\subsection{Hardy's Key Insight}

As Edwards \cite{edwards1974} emphasizes, Hardy's crucial insight was recognizing that if $\zeta(1/2 + it)$ had only finitely many zeros, certain integral transforms would exhibit monotonic behavior that contradicts their known analytic properties. This indirect approach circumvents the difficulty of directly analyzing individual zeros.

\begin{remark}[Edwards' Commentary]
Edwards notes that Hardy's proof, while not constructive, established a fundamental property of the zeta function that had eluded mathematicians since Riemann. The method introduced techniques that would later be refined by Hardy-Littlewood and Selberg to prove that a positive proportion of zeros lie on the critical line.
\end{remark}

\subsection{Quantitative Improvements}

Hardy's qualitative result has been significantly strengthened over the years:

\begin{theorem}[Selberg 1942 \cite{selberg1942}]
A positive proportion of the non-trivial zeros of $\zeta(s)$ lie on the critical line.
\end{theorem}

\begin{theorem}[Conrey 1989 \cite{conrey1989}]
At least $40\%$ of the non-trivial zeros of $\zeta(s)$ lie on the critical line $\Re(s) = 1/2$.
\end{theorem}

These improvements use refinements of Hardy's original method combined with more sophisticated estimates of exponential sums and moment calculations.

\subsection{Limitations of Hardy's Approach}

Despite its historical importance, Hardy's method has inherent limitations:

\begin{enumerate}
\item \textbf{Non-constructive Nature}: The proof doesn't provide a method to locate specific zeros on the critical line.
\item \textbf{Proportion vs. All}: Even the strongest results prove only that a certain proportion (currently 40\%) of zeros are on the critical line, not that all zeros are.
\item \textbf{Technical Barriers}: The method relies on average behavior rather than pointwise properties, making it difficult to extend to a proof of RH.
\end{enumerate}

As Edwards concludes, Hardy's theorem represents a major milestone in understanding the distribution of zeta zeros, but new ideas beyond this approach are needed to prove the full Riemann Hypothesis.

\section{The de Bruijn-Newman Constant}

One of the most significant recent developments in RH theory involves the de Bruijn-Newman constant, which provides a precise measure of how "barely true" the Riemann Hypothesis is.

\subsection{Definition and $H_t$ Functions}

\begin{definition}[de Bruijn-Newman Functions]
For $t \in \mathbb{R}$, define the functions:
\begin{equation}
H_t(z) = \int_0^{\infty} e^{tu^2} \Phi(u) \cos(zu) \, du
\end{equation}
where $\Phi(u)$ is the function defined by:
\begin{equation}
\xi(1/2 + iz) = 2\int_0^{\infty} \Phi(u) \cos(zu) \, du
\end{equation}
\end{definition}

The parameter $t$ acts as a "deformation" that affects the zero distribution of $H_t$.

\subsection{The Constant $\Lambda$}

\begin{definition}[de Bruijn-Newman Constant]
The \textbf{de Bruijn-Newman constant} $\Lambda$ is defined as:
\begin{equation}
\Lambda = \inf\{t \in \mathbb{R} : H_t \text{ has only real zeros}\}
\end{equation}
\end{definition}

This constant measures the critical threshold where all zeros become real.

\subsection{Connection to RH}

The profound connection between $\Lambda$ and the Riemann Hypothesis is:

\begin{theorem}[RH Equivalence]
The Riemann Hypothesis is equivalent to $\Lambda \leq 0$.
\end{theorem}

\begin{proof}[Proof Sketch]
The key insight is that $H_0$ is essentially equivalent to the xi function, and the deformation parameter $t$ can be thought of as "spreading out" the zeros. If $\Lambda > 0$, then for $t = 0$, the function $H_0$ must have some non-real zeros, which would correspond to zeros of $\xi(s)$ off the critical line.
\end{proof}

\subsection{Recent Progress: The "Barely True" Nature}

\begin{theorem}[Rodgers-Tao 2020 \cite{rodgerstao2020}]
$\Lambda \geq 0$.
\end{theorem}

This breakthrough result, combined with the known equivalence, shows that:

\begin{corollary}[Barely True Nature]
If the Riemann Hypothesis is true, then $\Lambda = 0$, meaning RH is "barely true" in the sense that it sits precisely at the boundary of truth.
\end{corollary}

\begin{remark}[Physical Interpretation]
The result $\Lambda \geq 0$ can be interpreted as saying that the zeta function's zeros are at the "edge of stability" - any perturbation in the wrong direction would create non-real zeros, violating RH.
\end{remark}

\section{The Lindelöf Hypothesis Connection}

The Lindelöf Hypothesis provides a growth condition that is weaker than RH but still captures important aspects of the zeta function's behavior.

\subsection{Statement of LH}

\begin{hypothesis}[Lindelöf Hypothesis]
For any $\epsilon > 0$:
\begin{equation}
\zeta(1/2 + it) = O(t^{\epsilon})
\end{equation}
as $t \to \infty$.
\end{hypothesis}

The conjectured truth is actually:
\begin{equation}
\zeta(1/2 + it) = O((\log t)^{2/3})
\end{equation}

\subsection{Relationship to RH}

The relationship between LH and RH is:

\begin{theorem}[RH Implies LH]
If the Riemann Hypothesis is true, then the Lindelöf Hypothesis holds.
\end{theorem}

However, LH does not imply RH, making it a weaker but potentially more accessible target.

\subsection{Growth Rate Implications}

The Lindelöf Hypothesis has profound implications for moments of the zeta function:

\begin{theorem}[Moment Connection]
If we could prove that for some fixed $k$:
\begin{equation}
\int_T^{2T} |\zeta(1/2 + it)|^{2k} dt = o(T(\log T)^{k^2})
\end{equation}
this would imply RH unconditionally.
\end{theorem}

For $k = 6$, this becomes:
\begin{equation}
\int_T^{2T} |\zeta(1/2 + it)|^{12} dt = o(T(\log T)^{36})
\end{equation}

\subsection{Current Status and Best Bounds}

The current best subconvexity bound is:

\begin{theorem}[Current Best Bound \cite{soundararajan2008}]
\begin{equation}
\zeta(1/2 + it) \ll t^{32/205 + \epsilon}
\end{equation}
\end{theorem}

\begin{remark}[Progress Stagnation]
This bound, while representing decades of work, remains far from the Lindelöf bound of $t^{\epsilon}$. Improvements have stalled despite intensive effort, suggesting fundamental barriers exist.
\end{remark}

\section{Zero Density Methods}

Zero density methods attempt to progressively improve zero-free regions until reaching the critical line.

\subsection{Zero-Free Regions}

Define the zero-free region:
\begin{equation}
R(\delta) = \{s = \sigma + it : \sigma > 1 - \delta(t), \zeta(s) \neq 0\}
\end{equation}

The goal is to find the largest possible function $\delta(t)$.

\subsection{Strategy for Improvement}

The strategy involves:
\begin{enumerate}
\item Establish zero-free regions using techniques like:
   \begin{itemize}
   \item Borel-Carathéodory theorem
   \item Phragmén-Lindelöf principle  
   \item Density arguments
   \end{itemize}
\item Progressively improve $\delta(t)$ through refined estimates
\item Ultimate goal: reach $\delta(t) = 1/2$ for all $t$, proving RH
\end{enumerate}

\subsection{Current Best Results}

The progression of results shows both progress and limitations:

\begin{theorem}[Classical Result]
There exists a constant $c > 0$ such that:
\begin{equation}
\delta(t) = \frac{c}{\log t}
\end{equation}
\end{theorem}

\begin{theorem}[Korobov-Vinogradov \cite{iwanieckowalski2004}]
The current best result is:
\begin{equation}
\delta(t) = \frac{c}{(\log t)^{2/3}(\log \log t)^{1/3}}
\end{equation}
\end{theorem}

\subsection{The Gap to RH}

\begin{remark}[Fundamental Barriers]
Despite steady progress over decades, current zero-free region methods face apparent barriers:
\begin{itemize}
\item The improvement from $\log t$ to $(\log t)^{2/3}$ required fundamentally new techniques
\item Further progress to reach $\delta = 1/2$ appears to require entirely different approaches
\item The gap between current methods and RH remains vast
\end{itemize}
\end{remark}

\section{Moment Methods and Random Matrix Theory}

The study of moments of the zeta function has revealed deep connections to random matrix theory and provided some of the strongest evidence for RH.

\subsection{Basic Principle}

Define the $2k$-th moment:
\begin{equation}
M_k(T) = \int_0^T |\zeta(1/2 + it)|^{2k} dt
\end{equation}

The behavior of these moments is intimately connected to the zero distribution.

\subsection{Keating-Snaith Conjectures}

Based on analogies with random matrix theory:

\begin{conjecture}[Keating-Snaith 2000 \cite{keatingsaith2000}]
\begin{equation}
M_k(T) \sim C_k T(\log T)^{k^2}
\end{equation}
where $C_k$ has an explicit formula derived from the theory of characteristic polynomials of random unitary matrices.
\end{conjecture}

The constants $C_k$ are given by:
\begin{equation}
C_k = \prod_{j=1}^k \frac{\Gamma(j)\Gamma(1+j)}{\Gamma(1+k)^2} \prod_{j=1}^k \frac{|\zeta(2j)|}{(2\pi)^j}
\end{equation}

\subsection{Connection to RH}

\begin{theorem}[Moment-RH Connection]
If the moments $M_k(T)$ grow more slowly than the conjectured rate for sufficiently large $k$, then zeros must lie on the critical line.
\end{theorem}

The intuition is that off-critical zeros would contribute additional growth to the moments.

\subsection{Known Results for Different Moments}

The status of moment calculations varies dramatically:

\begin{theorem}[First and Second Moments]
Asymptotic formulas are known:
\begin{align}
M_1(T) &\sim \frac{T}{2\pi} \log T \\
M_2(T) &\sim \frac{T}{2\pi^2} (\log T)^4
\end{align}
\end{theorem}

\begin{theorem}[Higher Moments]
\begin{itemize}
\item $k = 3$: Only upper bounds known, matching conjectured rate
\item $k \geq 4$: Conjectural formulas agree with numerical computations to remarkable precision
\item The agreement provides strong evidence for both RH and the random matrix connection
\end{itemize}
\end{theorem}

\begin{remark}[Computational Evidence]
Numerical verification of the Keating-Snaith predictions for $k \geq 4$ provides some of the most compelling evidence that the zeta function's zeros behave statistically like eigenvalues of random unitary matrices, where RH is automatically satisfied.
\end{remark}

\section{The Weil and Li Criteria}

Explicit formulas connecting zeros to arithmetic functions provide alternative characterizations of RH.

\subsection{Explicit Formula}

The starting point is the explicit formula connecting zeros to prime powers:

\begin{theorem}[Explicit Formula]
For suitable test functions $h$:
\begin{equation}
\sum_{\rho} h(\rho) = -\frac{1}{2\pi} \int_{-\infty}^{\infty} h(1/2 + it) \log|\zeta(1/2 + it)| \, dt + \text{explicit terms}
\end{equation}
where the explicit terms involve primes and trivial zeros.
\end{theorem}

\subsection{Weil's Positivity Criterion}

\begin{theorem}[Weil's Criterion]
The Riemann Hypothesis is equivalent to:
\begin{equation}
\sum_{\rho} h(\rho) \geq 0
\end{equation}
for all functions $h$ of the form $h(s) = |g(s)|^2$ where $g$ is an entire function of exponential type.
\end{theorem}

\begin{remark}[Intuition]
Weil's criterion transforms RH into a positivity condition. If zeros were off the critical line, certain test functions would produce negative sums, violating the criterion.
\end{remark}

\subsection{Li's Criterion with $\lambda_n$}

Li's criterion provides a more computational approach:

\begin{definition}[Li's Lambda Sequence]
Define:
\begin{equation}
\lambda_n = \sum_{\rho} \left[1 - \left(1 - \frac{1}{\rho}\right)^n\right]
\end{equation}
where the sum is over all non-trivial zeros $\rho$.
\end{definition}

\begin{theorem}[Li's Criterion \cite{li1997}]
The Riemann Hypothesis is equivalent to $\lambda_n \geq 0$ for all $n \geq 1$.
\end{theorem}

\begin{proof}[Proof Sketch]
The key insight is that if RH holds, the zeros $\rho = 1/2 + i\gamma$ have real part $1/2$, making the expression inside the sum have a specific sign structure that ensures positivity.
\end{proof}

\subsection{Computational Evidence}

Li's criterion has been extensively tested:

\begin{theorem}[Computational Verification]
The first several million values of $\lambda_n$ have been computed and found to be positive, with growth rates consistent with RH predictions.
\end{theorem}

The asymptotic behavior is:
\begin{equation}
\lambda_n \sim \frac{n}{2\pi^2} (\log n)^2
\end{equation}

\begin{remark}[Computational Significance]
While computational verification cannot prove RH, the consistency of millions of $\lambda_n$ values with RH predictions provides strong supporting evidence and has revealed no counterexamples.
\end{remark}

\section{Why Classical Approaches Have Not Succeeded}

Despite their sophistication and the deep insights they have provided, classical approaches to RH face fundamental obstacles.

\subsection{The Critical Strip is "Balanced"}

The functional equation $\zeta(s) = 2^s \pi^{s-1} \sin(\pi s/2) \Gamma(1-s) \zeta(1-s)$ creates a symmetry around $\Re(s) = 1/2$, but this symmetry alone does not force zeros to lie on the critical line.

\begin{remark}[Balance vs. Constraint]
The functional equation makes the critical line special but does not provide sufficient constraint to prove zeros lie there. Additional structural properties are needed.
\end{remark}

\subsection{Lack of Algebraic Structure}

Unlike polynomial equations, the zeta function lacks:
\begin{itemize}
\item Finite degree (it's a transcendental function)
\item Galois-theoretic structure
\item Algorithmic decidability
\end{itemize}

\begin{remark}[Transcendental Nature]
The transcendental nature of the zeta function means that classical algebraic techniques are insufficient, and new frameworks are needed.
\end{remark}

\subsection{The Problem is "Rigid"}

Small perturbations of the zeta function can destroy its key properties:

\begin{example}[Davenport-Heilbronn \cite{davenpoertheilbronn1936}]
The function:
\begin{equation}
f(s) = 5^{-s}[\zeta(s,1/5) + \tan\theta \zeta(s,2/5) - \tan\theta \zeta(s,3/5) - \zeta(s,4/5)]
\end{equation}
satisfies a functional equation similar to $\zeta(s)$ but has zeros off the critical line.
\end{example}

\subsection{Connection to Primes is Indirect}

While zeros control prime distribution, the relationship is transcendental rather than direct:

\begin{remark}[Arithmetic-Analytic Gap]
The gap between discrete arithmetic (primes) and continuous analysis (zeros) requires transcendental tools that current methods cannot fully bridge.
\end{remark}

\subsection{Current Methods Have Barriers}

Each classical approach faces specific obstacles:
\begin{itemize}
\item \textbf{Zero-free regions}: Logarithmic barriers in density methods
\item \textbf{Moments}: Cannot compute moments for large $k$
\item \textbf{Subconvexity}: Polynomial barriers to improvement
\item \textbf{Spectral methods}: Fundamental limitations identified by Bombieri-Garrett
\end{itemize}

\section{Conclusion}

The classical approaches to the Riemann Hypothesis have revealed profound structure in the zeta function and identified fundamental obstacles to proving RH. Key insights include:

\begin{enumerate}
\item The "barely true" nature of RH (de Bruijn-Newman constant $\Lambda \geq 0$)
\item Deep connections to random matrix theory
\item Multiple equivalent formulations providing different perspectives
\item Systematic computational evidence supporting RH
\item Fundamental theoretical barriers requiring new mathematical frameworks
\end{enumerate}

While these approaches have not yet yielded a proof, they have:
\begin{itemize}
\item Established the landscape of the problem
\item Identified what new tools might be needed
\item Provided overwhelming evidence for RH's truth
\item Revealed connections to other areas of mathematics
\end{itemize}

The failure of classical methods suggests that proving RH requires fundamentally new mathematical insights, possibly involving:
\begin{itemize}
\item Novel operator-theoretic constructions
\item Arithmetic quantum mechanical frameworks  
\item p-adic and tropical approaches
\item Synthesis of multiple viewpoints beyond current attempts
\end{itemize}

As Hilbert observed, the difficulty of RH may reflect that we are missing fundamental principles about the relationship between discrete arithmetic structures and continuous analytic objects. The classical approaches have mapped the territory; the proof awaits the discovery of new mathematical continents.

\section{Historical Failed Attempts and Lessons Learned}

The history of attempts to prove the Riemann Hypothesis provides crucial insights into why the problem has resisted solution for over 160 years. These failed attempts are not mere historical curiosities but essential data points revealing the fundamental difficulty of the problem.

\subsection{The Stieltjes Claim (1885)}

The first major claim of a proof came from Thomas Stieltjes, a respected Dutch mathematician known for the Riemann-Stieltjes integral.

\begin{historicalnote}
In 1885, Stieltjes claimed in a letter to Hermite and in a note to the Comptes Rendus of the Paris Academy that he had proved the Mertens conjecture, which would imply RH. He stated that his proof was ``very arduous'' and that he would ``try to simplify it further.'' The proof never appeared, and no trace of it was found in his papers after his death.
\end{historicalnote}

\begin{lesson}[Why Stieltjes Failed]
Stieltjes attempted to prove that $|M(x)|/\sqrt{x} < 1$ for all $x > 1$, where $M(x) = \sum_{n \leq x} \mu(n)$ is the Mertens function. This would have implied RH. However, Odlyzko and te Riele proved in 1985 that:
\begin{equation}
\limsup_{x \to \infty} \frac{M(x)}{\sqrt{x}} > 1.06
\end{equation}
Thus Stieltjes's approach was fundamentally flawed -- he was trying to prove a false statement.
\end{lesson}

\subsection{The Mertens Conjecture and Its Disproof}

The Mertens conjecture represents one of the most instructive failures in the history of RH.

\begin{conjecture}[Mertens, 1897]
For all $x \geq 1$:
\begin{equation}
|M(x)| < \sqrt{x}
\end{equation}
\end{conjecture}

This conjecture had substantial computational support:
\begin{itemize}
\item Verified for $x < 10^9$ by von Sterneck (1912)
\item No counterexample found despite extensive computation
\item Would have implied the Riemann Hypothesis
\end{itemize}

\begin{theorem}[Odlyzko-te Riele, 1985 \cite{odlyzko1985}]
The Mertens conjecture is false. Using the Lenstra-Lenstra-Lov\'asz lattice basis reduction algorithm, they proved:
\begin{equation}
\limsup_{x \to \infty} \frac{M(x)}{\sqrt{x}} > 1.06 \quad \text{and} \quad \liminf_{x \to \infty} \frac{M(x)}{\sqrt{x}} < -1.009
\end{equation}
\end{theorem}

\begin{warning}[Lesson from Mertens]
Computational evidence, even overwhelming evidence, can be misleading. The smallest counterexample to the Mertens conjecture is believed to be larger than $10^{20}$, possibly as large as $\exp(1.59 \times 10^{40})$. This demonstrates that:
\begin{enumerate}
\item Numerical verification alone cannot establish truth in number theory
\item Conjectures stronger than RH may be false even when RH is presumed true
\item Modern computational methods (lattice reduction) can disprove conjectures without finding explicit counterexamples
\end{enumerate}
\end{warning}

\subsection{Littlewood's Oscillation Result}

Littlewood's 1914 theorem revealed another case where intuition and numerical evidence were misleading.

\begin{theorem}[Littlewood, 1914]
The difference $\pi(x) - \Li(x)$ changes sign infinitely often as $x \to \infty$.
\end{theorem}

This contradicted the belief, held by Gauss and Riemann based on all available numerical evidence, that $\pi(x) < \Li(x)$ for all $x > 2$. The first sign change occurs at the astronomically large Skewes number.

\begin{definition}[Skewes Numbers]
\begin{itemize}
\item First Skewes number (1933, assuming RH): $x < e^{e^{e^{79}}}$
\item Second Skewes number (1955, unconditional): $x < 10^{10^{10^{963}}}$
\item Current best estimate: First sign change near $x \approx 1.397 \times 10^{316}$ \cite{plattrigaux2020}
\end{itemize}
\end{definition}

\subsection{Lessons from Historical Failures}

\begin{assessment}[Key Insights from Failed Attempts]
\begin{enumerate}
\item \textbf{Computational evidence is insufficient}: Both the Mertens conjecture and the belief that $\pi(x) < \Li(x)$ always had extensive computational support but were false.

\item \textbf{Stronger statements may be false}: Attempting to prove something stronger than RH (like Mertens) can lead to trying to prove false statements.

\item \textbf{Non-constructive proofs reveal structure}: Littlewood's oscillation theorem and the Odlyzko-te Riele disproof were existence proofs without explicit examples, yet revealed fundamental truths.

\item \textbf{Hidden manuscripts matter}: Siegel's 1932 discovery of Riemann's computational work in unpublished manuscripts showed that Riemann's ``probable'' was based on extensive calculation, not mere intuition.

\item \textbf{Progress is incremental}: From Hardy's 1914 proof \cite{hardy1914} of infinitely many zeros on the critical line to the current 41.28\%, progress has been slow but steady.
\end{enumerate}
\end{assessment}

\begin{philosophical}[The Value of Failed Attempts]
Failed attempts at proving RH have contributed more to our understanding than many successful theorems in other areas. They reveal:
\begin{itemize}
\item The inadequacy of current mathematical frameworks
\item The subtle interplay between computational evidence and theoretical truth
\item The necessity of new mathematical structures (operators, lattices, random matrices)
\item The depth of the arithmetic-analytic correspondence that RH encodes
\end{itemize}
Each failure eliminates not just one approach but entire classes of methods, progressively constraining the space of possible proofs and pointing toward the type of mathematics that will ultimately be required.
\end{philosophical}